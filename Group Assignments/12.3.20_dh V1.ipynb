{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UP206a Midterm by the Mangonadas: Andres and Dani\n",
    "Our research question: Are communities that are proximate to environmental health hazards more policed than those not exposed to environmental health hazards?\n",
    "\n",
    "This project intends to examine the intersections between proximity to environmental hazards and policng because environmental risks could indicate increased susceptibility to developing sociobehavioral issues deemed as deviant.\n",
    "\n",
    "#This question is important because polluted communities could be doubly vulnerable given their immediate social and physical conditions, making them subject to policing of their behaviors.\n",
    "\n",
    "#To explore our question, we are using census tract data from 2012, the LA City Geohub Environmental Justice Screening Method data, and crime data, to examine reports of arrest in Los Angeles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sodapy import Socrata\n",
    "import folium\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in EJSM df and make string of tracts to be named\n",
    "df = pd.read_csv(\n",
    "'Data/EJSM_Scores/EJSM_Scores (1).csv' ,\n",
    "dtype={\n",
    "    'Tract_1':str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add number zero leading the FIPS code for merging the data with the census tract data\n",
    "df['Tract_1'] = df['Tract_1'].str.zfill(11)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import 2012 census data\n",
    "tracts=gpd.read_file('Data/CensusData2012/census-tracts-2012.geojson')\n",
    "print(tracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List column names \n",
    "list(tracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select unwanted columns\n",
    "columns_to_drop = ['set','kind','resource_uri','metadata']\n",
    "#read columns \n",
    "tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns from tracts data\n",
    "tracts = tracts.drop(columns_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show tracts info\n",
    "tracts.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate the FIPS code and geometry columns to match with the EJSM data\n",
    "tracts = tracts[['name','geometry']]\n",
    "tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show isolated columns\n",
    "tracts.columns = ['FIPS','geometry']\n",
    "tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Tract_1 to FIPS\n",
    "df.columns = ['OBJECTID',\n",
    " 'FIPS',\n",
    " 'CIscore',\n",
    " 'HazScore',\n",
    " 'HealthScore',\n",
    " 'SVscore',\n",
    " 'CCVscore',\n",
    " 'Shape__Area',\n",
    " 'Shape__Length']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section will explore the merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List new df columns\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data on the same object FIPS\n",
    "tracts_ejsm=tracts.merge(df,on=\"FIPS\")\n",
    "#Show merge with census data\n",
    "tracts_ejsm.head()\n",
    "tracts_ejsm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_ejsm.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe stats by proximity to hazard score\n",
    "tracts_ejsm['CIscore'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram for cumulative impact score\n",
    "tracts_ejsm['CIscore'].plot.hist(bins=50),\n",
    "plt.xlabel('Cumulative Impact Scores')\n",
    "plt.ylabel('Frequency'),\n",
    "plt.savefig('CIscore.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram for proximity to hazard score. It is grouped by the number of occurences. \n",
    "#Our not so great graph! Seeing this bar chart intrigued us to durther explore the variations between census tracts.\n",
    "tracts_ejsm['HazScore'].plot.hist(bins=50)\n",
    "plt.xlabel('Proximity to Hazards')\n",
    "plt.ylabel('Frequecy'),\n",
    "plt.savefig('HazScore_bar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s unclear why the hazard score data is not distributed the same way as the cumulative impact score data. We wondered if it was because we had miswritten it or if the hazard occurrences were not visible on this bar graph. What is apparent is the average cumulative impact score of LA lying somewhere between 10.5-12.5. The cumulative score considers all the scores identified from the dataset included proximity to hazards, social vulnerability, climate vulnerability, and health vulnerability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New object to show the sorted data- these are the top polluted census tracts. The census tracts that are shown as being the most polluted are also the most socially vulnerable.\n",
    "tracts_ejsm_sorted = tracts_ejsm.sort_values(by='CIscore',ascending = False)\n",
    "tracts_ejsm_sorted[['FIPS','CIscore','HazScore','SVscore']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot represents the proximity to environmental hazards score by equal intervals \n",
    "#We can see that the most polluted areas are in south central LA county \n",
    "tracts_ejsm.plot(figsize=(24,20),\n",
    "                 column='HazScore',\n",
    "                 legend=True, \n",
    "                 scheme='equal_interval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot represents the cumulative impact scores separated by natural breaks\n",
    "#The map shows increased variation because of the integrated scores\n",
    "tracts_ejsm.plot(figsize=(24,20),\n",
    "                 column='CIscore',\n",
    "                 legend=True, \n",
    "                 scheme='NaturalBreaks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Folium map for HazScore. This score will be our main point of analysis\n",
    "\n",
    "m = folium.Map(location=[34.2,-118.2], \n",
    "               zoom_start = 9,\n",
    "               tiles='CartoDB positron', \n",
    "               attribution='CartoDB')\n",
    "\n",
    "# plot chorpleth over the base map\n",
    "folium.Choropleth(\n",
    "                  geo_data=tracts_ejsm, # geo data\n",
    "                  data=tracts_ejsm, # data          \n",
    "                  key_on='feature.properties.FIPS', # key, or merge column\n",
    "                  columns=['FIPS', 'HazScore'], # [key, value]\n",
    "                  fill_color='BuPu',\n",
    "                  line_weight=0.1, \n",
    "                  fill_opacity=0.8,\n",
    "                  line_opacity=0.2, # line opacity (of the border)\n",
    "                  legend_name='Degree of proximity to Environmenta Hazards)').add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Chloropleth map provides us with a background map that will kick off the rest of our work. We chose hazard scores because while the cumulative impact was meaningful, we wanted to focus on a standard that could be more decisively measured (proximity to environmental hazards in the form of pollutants) rather than other measurements. Measurements include a social vulnerability that could be more skewed based on indicators defined by the source (in this case, USC/Occidental College). For example, although poverty and policing can be linked, we wanted to see if sites that were contaminated or close to toxic sites (such as Exide) would have a significant relationship to policing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "There is a problem with the way you are importing the data. You have a limit of 10,000 records (this can be upped to 50,000), and you do not sort the data in any way. The resulting data is a subset of records of the total.\n",
    "    \n",
    "Consider instead to specify a date range and a query that makes the data fall under the 50000 record limit.\n",
    "\n",
    "I provide an example below (df2) for your reference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing our arrest 2020 data using an API\n",
    "client = Socrata(\"data.lacity.org\", None)\n",
    "results = client.get(\"amvf-fr72\", limit=10000)\n",
    "df2 =pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a more complete dataset\n",
    "results2 = client.get(\"amvf-fr72\", \n",
    "                     limit=50000,\n",
    "                     where = \"(arst_date between '2020-01-01T00:00:00' and '2020-10-30T00:00:00') AND (grp_description in ('Aggravated Assault','Miscellaneous Other Violations','Driving Under Influence Other Assaults','Narcotic Drug Laws', 'Disturbing the Peace', 'Disorderly Conduct'))\",\n",
    "                     order='arst_date desc')\n",
    "\n",
    "df2 =pd.DataFrame.from_records(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "This example limits the import to certain crime types (look at the list within the where statement). Adjust these as necessary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample data to check for inconsistencies\n",
    "df2.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get description of arrest data \n",
    "df2.grp_description.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining all data types in the df for floats\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "As you can see, the plot below paints a different picture.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plotly bar graph to examine the total arrests by LAPD in 2020\n",
    "px.bar(df2,\n",
    "       x='arst_date',\n",
    "       title='LAPD Arrests in 2020',\n",
    "       labels={'arst_date':'Arrest date','counts':'Number of arrests'}\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been a graph circulating, but it’s interesting to note how arrests boom in the summer months and the moments leading up to it due to the uprisings. What would these look like across time? When hearing about the ‘92 uprising, a familiar anecdote is how heat and power shortages aggravated communities already frustrated and fed up with the policing system. While it’s hard to test this, it’s important to note how many people, particularly folks of color, we’re stuck at home due to quarantine during summer, and many lacked air conditioning (or income to pay for utilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Index arrest charge value counts\n",
    "arrest_by_charge = df2.grp_description.value_counts().reset_index()\n",
    "arrest_by_charge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Again, these numbers will change with a more accurate import.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Index arrest charge value counts\n",
    "arrest_by_charge = df2.grp_description.value_counts().reset_index()\n",
    "arrest_by_charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make lat and lon floats \n",
    "df2['lat'] = df2['lat'].astype(float)\n",
    "df2['lon'] = df2['lon'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure geometry for df2 (crime) geodataframe\n",
    "df2 = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot of crime data\n",
    "df2.plot(figsize=(12,12),color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add base from tracts data\n",
    "base = tracts_ejsm.plot(figsize=(12,10),color='gainsboro', edgecolor='white')\n",
    "\n",
    "ax = df2.plot(ax=base, color='purple', markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print total bounds of data points \n",
    "minx, miny, maxx, maxy = df2.geometry.total_bounds\n",
    "print(minx)\n",
    "print(maxx)\n",
    "print(miny)\n",
    "print(maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print map with within closer boundaries df2\n",
    "base = tracts_ejsm.plot(figsize=(12,12),color='gainsboro', edgecolor='white')\n",
    "ax = df2.plot(ax=base, marker='o', color='purple', markersize=5)\n",
    "ax.set_xlim(minx - .1, maxx + .1)\n",
    "ax.set_ylim(miny - .1, maxy + .1)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure coordinate reference system for tracts to join the two dataframes\n",
    "tracts.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure coordinate reference system for crime\n",
    "df2.set_crs(epsg=4326, inplace=True)\n",
    "df2.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join tracts_ejsm and crime data\n",
    "join = gpd.sjoin(tracts,\n",
    "                 df2,\n",
    "                 how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the data from join\n",
    "df2 = pd.DataFrame(join,columns=['arst_date','rpt_id','grp_description','arst_typ_cd','HazScore', 'CIscore','FIPS','lat','lon','geometry'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset of the joined dataframes- need to figure out what went wrong in this subset\n",
    "#We are keeping these columns because our analysis focuses on the HazScore and description of the charges\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we follow Yoh’s steps more closely, but the aggravated arrests don’t tell us much. We’re trying to figure out crimes linked explicitly to environmental hazards. For now, we want to keep this large dataset because we want to be able to pick them out later depending on what crimes we want to analyze in the nexus with environmental hazards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the tracts with the highest number of arrests. We included this as a priliminary step to mapping the full counts. \n",
    "crime_by_tracts = join.FIPS.value_counts().rename_axis('Tract').reset_index(name='crime_count')\n",
    "crime_by_tracts.head()\n",
    "\n",
    "df2="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is a visualization to compare the amount of arrests in different census tracts; df2 using yoh's subset\n",
    "crime_by_tracts[:50].plot.bar(figsize=(20,8),x='Tract',y='crime_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar graph tells us the crime by census tract, but we can only guess what that is. It would be useful to bring this into the function, but we’re looking for a better way of organizing and visualizing the data. Instead of looking at specific census tracts themselves, our goal is to understand the interconnected relationship between proximity to hazards (such as pollution) and seeing what charges the police are giving people in that area. Surprisingly, the census tract with the most crimes (06037980028) is the one belonging to the airport and in between the Chevron Oil Refinery and the Ballon Wetlands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DF IS MISSING FIPS- Grouped data by the description of arrest. Here, the FIPS out of order because we are descibing a full count of charge descriptions. \n",
    "df.groupby(['grp_description']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO REVISIT Group data by the FIPS code to explore the meaning of the counts in each census tract. \n",
    "df.groupby(['FIPS']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is grouped for counts of reports in tracts. \n",
    "df_grouped=df.groupby(['FIPS','grp_description','lat','lon','arst_date']).count()[['rpt_id']]\n",
    "df_grouped.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten data to prep for bar graph and reset the index to include the group values \n",
    "df_flat = df_grouped.reset_index()\n",
    "df_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how beautiful that index is. What is not shown in our flattened data frame is the amount of blood and tears (and lots of green tea caffeine) that helped us get to this point. Originally while our workflow was simple, things became more complicated as we tried to clean up our notebooks without taking away code that would serve us moving forward. This data frame allows us to see all types of crimes and arrest data across census tracts and their location. This provides us with the top-most layer for our future maps, in addition to a versatile data frame that can give temporal nuance to our maps, an attribute especially useful for the Kepler map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Modified the code below (added marker_line_width=0) to get rid of the white horizontal lines that were making the bars difficult to read.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make plotly bar graph\n",
    "fig = px.bar(df_flat,\n",
    "       x='grp_description',\n",
    "       y='rpt_id',\n",
    "       title='Description of LAPD Arrests in 2020',\n",
    "       color='grp_description',\n",
    "       labels={'grp_description':'Arrest Decription','rpt_id':'Number of Arrests'}\n",
    "      )\n",
    "fig.update_traces(marker_line_width=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. It was tricky to make sense of all these numbers, but having everything cleanly indexed would provide us more than enough for what we needed. Looking back now, we would like to have examined a larger sample, but based on our gigabyte limit on Jupyter Hub, we were unsure how far to push this. From this bar graph, it becomes clear that aggravated assault, driving under the influence, and other crimes are the largest ones. We’re trying to focus on aggravated assault, disturbing the peace, and disorderly contact, because these are the ones we believe can connect policing to proximity to hazards (the ejsm score). It’s clear that aggravated assault is the largest sample; there are significantly less for disorderly conduct and disturbing the peace. For now, we don’t know if this will affect the statistical significance of our findings later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the merged dataframe\n",
    "df_ejsm = tracts_ejsm.merge(df_flat,on=\"FIPS\")\n",
    "df_ejsm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert lat and lon to floats intergrating arrest and EJSM score data\n",
    "df_ejsm['lat'] = df_ejsm['lat'].astype(float)\n",
    "df_ejsm['lon'] = df_ejsm['lon'].astype(float)\n",
    "df_ejsm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe to crs\n",
    "df_ejsm.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create basic scatter of df_ejsm_crime_tracts data with coordinate reference system\n",
    "px.scatter(df_ejsm,\n",
    "           x='lon',\n",
    "           y='lat'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring map styles using mapbox\n",
    "fig = px.scatter_mapbox(df_ejsm,\n",
    "                        lat='lat',\n",
    "                        lon='lon',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make temporal scatter plot map\n",
    "fig = px.scatter_mapbox(df_ejsm, \n",
    "                        lat=\"lat\", \n",
    "                        lon=\"lon\", \n",
    "                        color=\"grp_description\",\n",
    "                        animation_frame = 'arst_date',\n",
    "                       )\n",
    "fig.update_layout(mapbox_style=\"carto-darkmatter\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kepler \n",
    "from keplergl import KeplerGl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make base map from reference\n",
    "map = KeplerGl(height=600,width=800)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracts_ejsm have no attribute string\n",
    "map.add_data(data=df_ejsm,name='EJSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#At this point, we were stumped. We were clueless as to how our Kepler map was not fully functioning. In previous iterations, we were able to add on our merged data, but in this case, something has occurred in between our workflows where we couldn’t figure out the discrepancy. The first time around, we thought the geometry was converted to an object instead of remaining as geometry, and we were able to correct this. But even then, Kepler went unchanged. Though it had been simple, something occurred when we merged notebooks, and we still haven’t figured out why df_ejsm won’t work on Kepler even though it has a geometry file and the coordinates have been converted appropriately. After we went back, we realized we had merged the wrong tract files, and once we corrected ejsm_tracts to tracts, the rest of the notebook fell into place.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_tracts = tracts_ejsm[['FIPS','HazScore','geometry']]\n",
    "census_tracts.head()\n",
    "census_tracts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_tracts = census_tracts.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use census tracts and ejsm integrated layers to add base map\n",
    "ax=census_tracts.plot(figsize=(20,20), \n",
    "                      edgecolor='white',\n",
    "                      column='HazScore',\n",
    "                      alpha=0.5)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "ctx.add_basemap(ax,source=ctx.providers.CartoDB.Positron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make crime GeoDataFrame\n",
    "crime = gpd.GeoDataFrame(df_flat, \n",
    "                         crs='EPSG:4326',\n",
    "                         geometry=gpd.points_from_xy(df_flat.lon, df_flat.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert crime to crs\n",
    "crime = crime.to_crs(epsg=3857)\n",
    "crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use geometry to define area\n",
    "crime.geometry.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print total bounds\n",
    "minx, miny, maxx, maxy = crime.geometry.total_bounds\n",
    "print(minx)\n",
    "print(maxx)\n",
    "print(miny)\n",
    "print(maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure function map by census tracts as the base map\n",
    "base = census_tracts.plot(figsize=(20,20), \n",
    "                      edgecolor='white',\n",
    "                      column='HazScore',\n",
    "                      alpha=0.5)\n",
    "\n",
    "ax = crime.plot(ax=base, marker='o', color='purple', markersize=5)\n",
    "\n",
    "ax.set_xlim(minx - 7000, maxx + 7000) \n",
    "ax.set_ylim(miny - 7000, maxy + 7000)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "ctx.add_basemap(ax,source=ctx.providers.CartoDB.Positron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define bounds for the map using the function\n",
    "crime.geometry.total_bounds\n",
    "minx, miny, maxx, maxy = crime.geometry.total_bounds\n",
    "print(minx)\n",
    "print(maxx)\n",
    "print(miny)\n",
    "print(maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_arrests_by_type(charge='Aggravated Assault'):\n",
    "    \n",
    "    crime_type = crime[crime.grp_description==charge]\n",
    "\n",
    "    minx, miny, maxx, maxy = crime_type.geometry.total_bounds\n",
    "    \n",
    "    arrests_in_tracts = gpd.sjoin(census_tracts,crime_type,how='right')\n",
    "\n",
    "    base = census_tracts.plot(figsize=(20,20), \n",
    "                    edgecolor='white',\n",
    "                    column='HazScore',\n",
    "                    alpha=0.5)\n",
    "\n",
    "    ax = arrests_in_tracts.plot(ax=base, \n",
    "                                    column='grp_description',\n",
    "                                    marker='o',\n",
    "                                    markersize=40, \n",
    "                                    legend=True,\n",
    "                                    cmap='tab20',\n",
    "                                    legend_kwds={\n",
    "                                       'loc': 'upper right',\n",
    "                                       'bbox_to_anchor':(1.3,1)\n",
    "                                    }                  # this puts the legend to the side\n",
    "                                )\n",
    "\n",
    "    ax.set_xlim(minx - 200, maxx + 200) # added/substracted value is to give some margin around total bounds\n",
    "    ax.set_ylim(miny - 200, maxy + 200)\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.set_title('Occurences of '+crime.grp_description.values[0]+' in Los Angeles',fontsize=20)\n",
    "\n",
    "    ctx.add_basemap(ax,source=ctx.providers.CartoDB.Positron)\n",
    "    ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Map arrests by using the function\n",
    "map_arrests_by_type(charge='Aggravated Assault')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we reached a dilemma. After extensive back and forth, we were unable to get the map to filter the crime points by type. Instead, it highlighted all the census tracts where these crimes had occurred. We had to stop and reassess our workflow since we weren’t making any progress and repeating previous issues. Our goal for our function was for the large map to show all the points we wanted so that rather than zooming in on a place, we instead see where crimes were committed by type and identify where certain crimes occurred more than others. Alas, we were still able to understand more. In previous iterations, we created a line of code using \".isin\", though we also found that it aggregated the crimes we were trying to isolate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "The issue with the map above was that you had the spatial join be an \"inner\" join, which prioritizes the census tract polygons (thus coloring them in). Instead, use \"right\", which prioritizes the crime data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division of labor\n",
    "DH: Finalized notebooks, explored data, imported maps, coordinated with Andres to troubleshoot, indexed data and dataframes\n",
    "AG: Visualized some of the interactive maps, including Kepler; co-ordinated with Dani to troubleshoot function; Multiple overlays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
